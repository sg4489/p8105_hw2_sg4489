---
title: "p8105_hw2_sg4489"
author: "sg4489"
date: "2024-09-30"
output: html_document
---

```{r import_packages, warning=FALSE, message=FALSE}
# Import required packages
library(tidyverse)
library(readxl)
library(dplyr)
library(knitr)
```

# Problem 1: NYC Transit data

```{r Problem1_general}
# Read and clean the data
transit_data <- read_csv(
  "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", show_col_types = FALSE) %>% 
  select(line = 'Line', 
         station_name = 'Station Name', 
         station_latitude = 'Station Latitude', 
         station_longitude = 'Station Longitude', 
         Route1, Route2, Route3, Route4, Route5, Route6, Route7, Route8, Route9, Route10, Route11,  
         entry = Entry, 
         vending = Vending, 
         entrance_type = 'Entrance Type', 
         ada_compliance = ADA) %>% 
  # Convert the entry variable from character (YES vs NO) to a logical variable
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) %>% 
  unite("routes_served", Route1:Route11, sep = ",", na.rm = TRUE, remove = TRUE) 

# Dimensions of the cleaned dataset
transit_data_summary <- transit_data %>% 
  summarise(rows = n(), columns = ncol(.))
```

The resulting dataset has a dimension of `r transit_data_summary$rows` rows and `r transit_data_summary$columns` columns.

This dataset contains information about the entrances and exits of NYC subway stations, with variables including the subway line(`line`), station name(`station_name`), station latitude and longtitude(`station_latitude` and `station longtitude`), routes served(`routes_served`), whether entry is allowed(`entry`), vending machine availability(`vending`), entrance type(`entrance_type`), and ADA compliance(`ada_compliance).  

The data cleaning steps included selecting columns, renaming them for clarity, and convert the entry variable(`entry`) from character (YES vs NO) to a logical variable(`TRUE`/`FALSE`).  

The dataset is tidy because each row represent a single entrance/exit, each colum is a distinct variable, and each cell contains a single value.

## Answer the following questions using these data:

### How many distinct stations are there?

```{r Problem1_Q1}
distinct_stations <- transit_data %>% 
  distinct(line, station_name) %>% 
  count()
```

There are `r distinct_stations` distinct stations.  

### How many stations are ADA compliant?

```{r Problem1_Q2}
ada_compliant_stations <- transit_data %>% 
  filter(ada_compliance == TRUE) %>% 
  distinct(line, station_name) %>% 
  count()
```

There are `r ada_compliant_stations` ADA compliant stations.  

### What proportion of station entrances / exits without vending allow entrance?

```{r Problem1_Q3}
entrances_without_vending <- transit_data %>% 
  filter(vending == "NO" & entry == TRUE) %>% 
  count()

total_no_vending <- transit_data %>% 
  filter(vending == "NO") %>%
  count()

no_vending_entrance_propotion <- entrances_without_vending / total_no_vending
```

The propotion of station entrances / exits without vending allow entrance is `r no_vending_entrance_propotion*100` %.  

## Reformat data so that route number and route name are distinct variables.

```{r reformat}
transit_data_reformat <- transit_data %>% 
  mutate(
    route_number = ifelse(str_detect(routes_served, "[0-9]+$"), routes_served, NA),
    route_name = ifelse(str_detect(routes_served, "[A-za-z]+$"), routes_served, NA),
  )
```

In this way, the route name and route number are separated.  

### How many distinct stations serve the A train?

```{r Problem1_Q4}
transit_data_long <- transit_data %>%
  separate_rows(routes_served, sep = ",") %>%
  distinct(line, station_name, routes_served, .keep_all = TRUE)

stations_serving_A <- transit_data_long %>%
  filter(routes_served == "A") %>%
  distinct(station_name, line) %>%
  count()
```

There are `r stations_serving_A` stations serve the A train.  

### Of the stations that serve the A train, how many are ADA compliant?

```{r Problem1_Q5}
ada_compliant_A_stations <- transit_data_long %>%
  filter(routes_served == "A", ada_compliance == TRUE) %>%
  distinct(station_name, line) %>%
  count()
```

`r ada_compliant_A_stations` stations serve the A train are ADA compliant.  

# Problem 2: Mr. Trash Wheel dataset 

## Read and clean datasets 

### Read Mr. Trash Wheel data 

```{r, warning=FALSE, message=FALSE}
mr_trash_wheel <- suppressMessages(
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1, 
    col_types = c("text", "text", "numeric", "text", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric")) %>% 
    janitor::clean_names() %>% # Clean and normalize column names
    filter(!is.na(dumpster)) %>% # Filter out lines without "dumpster" information
    mutate(
      trash_wheel = "Mr. Trash Wheel",
      # Round and converts the number of sports balls to nearest integer variable
      sports_balls = as.integer(round(sports_balls, 0))))

# Detection and remove columns with NA in mr_trash_wheel
if (all(is.na(mr_trash_wheel$x15)) && all(is.na(mr_trash_wheel$x16))) 
  mr_trash_wheel <- mr_trash_wheel %>% select(-x15, -x16)
```

### Read Professor Trash Wheel data 

```{r}
prof_trash_wheel <- read_excel(
  "data/202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1, 
  col_types = c("text", "text", "numeric", "text", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric")) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(
    trash_wheel = "Professor Trash Wheel") 
```

### Read Gwynnda Trash Wheel data 

```{r}
gwynnda_trash_wheel <- read_excel(
  "data/202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1, 
  col_types = c("text", "text", "numeric", "text", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric")) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(
    trash_wheel = "Gwynnda Trash Wheel")
```

## Merge datasets

```{r}
# View the remain column names
mr_cols <- colnames(mr_trash_wheel)
prof_cols <- colnames(prof_trash_wheel)
gwynnda_cols <- colnames(gwynnda_trash_wheel)

# Find common columns of three datasets
common_cols <- Reduce(
  intersect,list(mr_cols, prof_cols, gwynnda_cols))
cat("Common columns of the three sheet are: ", paste(common_cols, collapse = ", "), "\n")

# Find the difference between Mr. Trash Wheel and other datasets
diff_mr_prof <- setdiff(mr_cols, prof_cols)
diff_mr_gwynnda <- setdiff(mr_cols, gwynnda_cols)
cat("The different columns between mr_trash_wheel and prof_trash_wheel are: ", 
    paste(diff_mr_prof, collapse = ","), "\n")
cat("The different columns between mr_trash_wheel and gwynnda_trash_wheel are: ", 
    paste(diff_mr_gwynnda, collapse = ","), "\n")

# Merge three datasets
combined_trash_wheel <- bind_rows(mr_trash_wheel, prof_trash_wheel, gwynnda_trash_wheel)

# Move the trash wheel to the first column for easier viewing
combined_trash_wheel <- combined_trash_wheel %>% select(trash_wheel, everything())
```
## Describe data

```{r}
# Get the number of observations in the result dataset
total_observations <- nrow(combined_trash_wheel)

# Calculate the total weight of trash collected by Professor Trash Wheel
total_weight_prof <- combined_trash_wheel %>% 
  filter(trash_wheel == "Professor Trash Wheel") %>% 
  summarise(total_weight = sum(weight_tons, na.rm = TRUE)) %>% 
  pull(total_weight)

# Calculate the total number of cigarette butts collected by Gwynnda in June 2022
cigarette_butts_gwynnda_2022 <- combined_trash_wheel %>% 
  filter(trash_wheel == "Gwynnda Trash Wheel" & year == 2022, month == "June") %>% 
  summarise(total_cigarette_butts = sum(cigarette_butts, na.rm = TRUE)) %>% 
  pull(total_cigarette_butts)
```

This dataset contains a total of `r total_observations` observations, with key variables such as year and month (indicating time), weight_tons (weight of trash in tons), and cigarette_butts (number of cigarette butts). Professor Trash Wheel collected a total of `r total_weight_prof` tons of trash, while Gwynnda collected `r format(cigarette_butts_gwynnda_2022, scientific = FALSE)` cigarette butts in June 2022. These data provide useful insights into the trash collected by different Trash Wheels over time.  

# Problem 3: Great British Bake Off

## Part I

### Import, clean, tidy, and otherwise wrangle each of these datasets

```{r import_dataset}
# Import datasets
bakers <- read_csv("data/gbb_datasets/bakers.csv", show_col_types = FALSE)
bakes <- read_csv("data/gbb_datasets/bakes.csv", show_col_types = FALSE)
results <- read_csv("data/gbb_datasets/results.csv", skip = 2, show_col_types = FALSE)
```

1. Cleaning and organizing the bakers dataset and splitting the baker's name into first and last Names. This way, when merging data sets, we can better match the other two data sets.  

```{r}
# For bakers: 
bakers <- bakers %>% 
  janitor::clean_names() %>% 
  arrange(series)

# Separate the first name and last name of the Baker Name in the baker 
bakers <- bakers %>%
  separate(`baker_name`, into = c("first_name", "last_name"), sep = " ", extra = "merge")
```

2. Cleaning and organizing the bakes dataset. Similar to the bakers dataset, this cleans the column names and sorts the bakes data by the series column. The person who is "Jo" in this dataset appears as Jo in the other two datasets, so we want to remove "".  

```{r}
# For bakes: 
bakes <- bakes %>% 
  janitor::clean_names() %>% 
  arrange(series)

# Remove the "" in the baker column
bakes <- bakes %>%
  mutate(baker = str_replace_all(baker, '"', ''))
```

3. Cleaning and organizing the results dataset. Again, the results dataset undergoes the same cleaning and sorting process using clean_names() and arrange(series).  

```{r}
# For results: 
results <- results %>% 
  janitor::clean_names() %>% 
  arrange(series)
```

This prepares the data to be merged or used in subsequent analysis steps, ensuring that the datasets have a consistent format and structure.  

### Check for completeness and correctness across datasets

We use anti_join(x, y, by = ...): Finds rows in x that do not have matching rows in y.  

1. Identify discrepancies between results and bakes.  

missing_in_results: Identifies bakers and episodes present in bakes but not in results.  
 
```{r}
missing_in_results <- anti_join(bakes, results, by = c("series", "episode", "baker"))
missing_in_results
```

missing_in_bakers: Finds bakers listed in results but not present in the bakers dataset.   

```{r}
missing_in_bakers <- anti_join(results, bakers, by = c("series", "baker" = "first_name"))
missing_in_bakers
```

2. Identify discrepancies between results and bakers.  

Find rows in results that have bakers not present in the bakers dataset.  

```{r}
missing_in_bakers <- anti_join(results, bakers, by = c("series", "baker" = "first_name"))
missing_in_bakers
```

Find rows in bakers that are not present in results.  

```{r}
missing_in_results_for_bakers <- anti_join(bakers, results, by = c("series", "first_name" = "baker"))
missing_in_results_for_bakers
```

3. Identify discrepancies between bakes and bakers.  

Find rows in bakes that have bakers not present in the bakers dataset.  

```{r}
missing_bakers_in_bakes <- anti_join(bakes, bakers, by = c("series", "baker" = "first_name"))
missing_bakers_in_bakes
```

Find rows in bakers that are not present in bakes.  

```{r}
missing_in_bakes_for_bakers <- anti_join(bakers, bakes, by = c("series", "first_name" = "baker"))
missing_in_bakes_for_bakers
```

### Merge to create a single, final dataset and organize this so that variables and observations are in meaningful orders

Now let's combines multiple datasets (results, bakes, and bakers) into a single, unified dataset named final_data.  

1. Combining results and bakes using full_join. The full_join merges series, episode, and baker. This means all rows from both datasets are included, even if there isn't a perfect match, and missing values will be filled with NA.  

```{r}
# Combine results and bakes
combined_data <- results %>%
  full_join(bakes, by = c("series" = "series", "episode" = "episode", "baker" = "baker"))
```

2. Combining combined_data with bakers. Another full_join is used, this time merging on the columns series and baker (matching first_name in the bakers dataset). After the merge, the baker column from combined_data is renamed to first_name for clarity and consistency.  

```{r}
# Continue combine bakers dataset
final_data <- combined_data %>%
  full_join(bakers, by = c("series" = "series", "baker" = "first_name")) %>% 
  rename(first_name = baker)
```

3. Organizing the columns in a meaningful order. The resulting final_data dataset contains all relevant details about each contestant, their performance, and demographic information, organized in a way that's ready for further analysis or reporting.  

```{r}
# Organize variables and observations in meaningful orders
final_data <- final_data %>%
  select(series, episode, first_name, last_name, baker_occupation, baker_age, hometown, 
         technical, result, signature_bake, show_stopper)
```

4. Export the final_data.  

```{r export}
# Export the final_data as a CSV in the directory containing the original dataset
write_csv(final_data, file.path(getwd(), "data","gbb_datasets","final_data.csv"))
```

### Briefly discuss the final dataset.

The final_data dataset is a comprehensive combination of the results, bakes, and bakers datasets from the Great British Bake Off. It captures detailed information about each contestant's participation across all seasons and episodes.  
Key Points:  
1. Structure:    
  The dataset consists of multiple columns capturing essential information for each baker and their performance across different episodes and seasons.  
  The columns include series (season number), episode (episode number), first_name, last_name (baker's name), baker_occupation, baker_age, hometown, technical (technical challenge rank), result (whether the baker was in, out, etc.), signature_bake (description of their signature bake), and show_stopper (description of their showstopper bake).  
2. Data Organization:  
  The dataset is sorted by series and episode, making it easy to track the progress of each season and episode sequentially.  
  All relevant data is combined into a single data frame, eliminating the need to reference multiple data sources.  
3. Completeness:   
  The full_join operations ensured that no data was lost, even if some entries were missing in one or more original datasets. This means that the dataset will have NA values where data was not available for certain bakers or episodes, preserving all possible information.  
4. Usability:  
  The final_data dataset is ready for further analysis, such as identifying trends, comparing the performance of different bakers, and visualizing the distribution of "Star Baker" winners across seasons.  
  The dataset is well-prepared for generating insights into how different bakers performed in signature, technical, and showstopper challenges.  

## Part II

### Create a table

```{r create_table}
# Filter for winners in Seasons 5 to 10
season_5_to_10 <- final_data %>%
  filter(series >= 5 & series <= 10) %>%
  select(series, episode, first_name, result) %>%
  filter(result == "WINNER" | result == "STAR BAKER") %>%
  arrange(series, episode)

# Create a table for Winners from Season 5 to 10
kable(
  season_5_to_10,
  col.names = c("season", "episode", "bakers", "result"),
  caption = "Star Baker and Winners from Season 5 to 10")
```

### Comment on this table

```{r count}
# Find the bakers with the most "Star Baker" titles in each season
most_star_bakers <- season_5_to_10 %>% 
  filter(result == "STAR BAKER") %>% 
  group_by(series, first_name) %>% 
  summarise(star_baker_count = n()) %>% 
  arrange(series, desc(star_baker_count)) %>% 
  slice(1) %>% 
  ungroup() 

# Find the winner of each season
season_winners <- season_5_to_10 %>% 
  filter(result == "WINNER") %>% 
  select(series, first_name) %>% 
  arrange(series) 
```

Season 5: Richard won "Star Baker" five times, which made him a strong contender, but Nancy ultimately won in the final episode, which was somewhat surprising.  
Season 6: Nadiya won "Star Baker" multiple times and was crowned the winner, indicating a predictable path to victory.  
Season 7: Candice showed strong performance, winning "Star Baker" four times and the final episode, indicating a predictable win.  
Season 8: Sophie won "Star Baker" three times and became the winner, showing a relatively predictable outcome.  
Season 9: Rahul's repeated success early in the season and eventual win made him a predictable winner.  
Season 10: Steph was a dominant contestant with four "Star Baker" titles, but David won the final, which was a surprise.  

Surprising Outcomes:  
Season 5: Richard's five-time "Star Baker" performance didn't lead to a win, which was unexpected as he was the strongest contender throughout the season.  
Season 10: Steph's four-time "Star Baker" titles indicated she was the favorite, but David's win was unexpected, showcasing that the final challenge can change the outcome drastically.  

Comments on the Table:  
The table reveals that being consistently named "Star Baker" doesn't always guarantee a win, as seen with Richard (Season 5) and Steph (Season 10).  
There were seasons with more predictable outcomes, like Seasons 6, 7, and 9, where the ultimate winners showed strong, consistent performances throughout.  


## Part III

### Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset.  

```{r data_process}
# Import dataset
viewers <- read_csv("data/gbb_datasets/viewers.csv", show_col_types = FALSE)

# View the structure of the data
str(viewers)

# Clean and check cleaned column names
viewers <- viewers %>% janitor::clean_names()
colnames(viewers)

# Convert all "N/A" or "NA" strings to actual NA values
viewers[viewers == "N/A" | viewers == "NA"] <- NA

# Show the first 10 rows of this dataset
head(viewers, 10)
```

### What was the average viewership in Season 1? In Season 5?

```{r calculate_average}
# Calculate the average viewership of season 1
average_viewership_season1 <- viewers %>%
  summarise(avg_viewership = mean(`series_1`, na.rm = TRUE))
cat("The average viewership of season 1 is: ", average_viewership_season1$avg_viewership, "\n")


# Calculate the average viewership of season 5
average_viewership_season5 <- viewers %>%
  summarise(avg_viewership = mean(`series_5`, na.rm = TRUE))
cat("The average viewership of season 5 is: ", average_viewership_season5$avg_viewership, "\n")
```
